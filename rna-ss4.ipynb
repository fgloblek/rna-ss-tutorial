{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from torchmetrics import Accuracy, Precision, Recall, F1Score, AUROC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasta_to_ss(input_folder, input_fasta, output):\n",
    "    \n",
    "    with open(input_fasta, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    lines_to_write = ''\n",
    "    for line in lines:\n",
    "        if line.startswith('>'):\n",
    "            file_name = line.split()[1]\n",
    "            file_path = os.path.join(input_folder, file_name)\n",
    "            with open(file_path, 'r') as f:\n",
    "                ctlines = f.readlines()\n",
    "                sequence = ''.join( ctline.split()[1] for ctline in ctlines[1:] )\n",
    "                pairings = [ ( ctline.split()[0],ctline.split()[4]) for ctline in ctlines[1:]]\n",
    "        \n",
    "            pairs = [ a+'-'+b for a,b in pairings if int(b)!=0 ]\n",
    "            ss = ' '.join(pairs)\n",
    "            lines_to_write += line.split()[0][1:] +' '+ ss + '\\n'\n",
    "                \n",
    "    with open(output,'w') as f:\n",
    "        f.write(lines_to_write)\n",
    "        \n",
    "fasta_to_ss('archiveII', 'archiveII.fasta', 'ss.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_csv(target_file, input_fasta, split_dir, split_file, output):\n",
    "    with open( os.path.join(os.getcwd(),split_dir,split_file), 'r') as f:\n",
    "        split_set = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "    with open(target_file,'r') as f:\n",
    "        lines = f.readlines()\n",
    "        target_dict = {}\n",
    "        for line in lines:\n",
    "            pairs = ' '.join(line.split()[1:])\n",
    "            target_dict[ line.split()[0] ] = pairs\n",
    "        \n",
    "    sequence_dict = {}\n",
    "    with open(input_fasta,'r') as f:\n",
    "        for line in f.readlines():\n",
    "            if line.startswith('>'):\n",
    "                seq_name = line.split()[0][1:]\n",
    "            else: \n",
    "                sequence = line.strip()\n",
    "                sequence_dict[seq_name]=sequence\n",
    "                \n",
    "    with open( os.path.join(os.getcwd(),split_dir,output),'w') as f:\n",
    "        for seq in split_set:\n",
    "            f.write(seq+','+sequence_dict[seq]+','+target_dict[seq]+'\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_csv('ss.txt', 'archiveII.fasta', 'data_splits','test.txt', 'test_ss.csv')\n",
    "make_csv('ss.txt', 'archiveII.fasta', 'data_splits','validation.txt', 'validation_ss.csv')\n",
    "make_csv('ss.txt', 'archiveII.fasta', 'data_splits','train.txt', 'train_ss.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset, one-hot encoding\n",
    "class RNADataset(Dataset):\n",
    "    def __init__(self, file_path,MAX_LENGTH = 500):\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self.MAX_LENGTH = MAX_LENGTH\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    _,seq,pairs = line.strip().split(',')\n",
    "                    if len(seq) > self.MAX_LENGTH: \n",
    "                        continue\n",
    "                    self.data.append(self._encode_sequence(seq))\n",
    "                    try:\n",
    "                        self.labels.append(self._decode_label(pairs,seq))\n",
    "                    except ValueError:\n",
    "                        print(line)\n",
    "\n",
    "    def _encode_sequence(self, sequence):\n",
    "        tokenizer = {'A': 0, 'C': 1, 'G': 2, 'U': 3}\n",
    "        return torch.tensor([tokenizer[c] for c in sequence])\n",
    "    \n",
    "    def _decode_label(self, pairs,sequence):\n",
    "        L = len(sequence)\n",
    "        a = np.zeros((L,L))\n",
    "        if pairs != '':\n",
    "            keys = [ list( map(int,pair.split('-')) ) for pair in pairs.split(' ') ]\n",
    "            for i,j in keys:\n",
    "                if j!=0:\n",
    "                    a[i-1,j-1] = 1\n",
    "                    a[j-1,i-1] = 1\n",
    "        return torch.tensor( a, dtype=torch.float32 )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# lightning datamodule\n",
    "class RNADataModule(pl.LightningDataModule):\n",
    "    def __init__(self, train_file, val_file, test_file, batch_size=32):\n",
    "        super().__init__()\n",
    "        self.train_file = train_file\n",
    "        self.val_file = val_file\n",
    "        self.test_file = test_file\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset = RNADataset(self.train_file)\n",
    "        self.val_dataset = RNADataset(self.val_file)\n",
    "        self.test_dataset = RNADataset(self.test_file)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testint = 'sequence1818,GGGCCCGUCGUCUAGCCUGGCUAAGAUGCGGGGUACGGGACCCCGUGGUCCGGGGUUCAAAUCCCCGCGGGCCCACCA,1-74 2-73 3-72 4-71 5-70 6-69 7-68 10-27 11-26 12-25 13-24 24-13 25-12 26-11 27-10 29-45 30-44 31-43 32-42 33-41 41-33 42-32 43-31 44-30 45-29 51-67 52-66 53-65 54-64 55-63 63-55 64-54 65-53 66-52 67-51 68-7 69-6 70-5 71-4 72-3 73-2 74-1'\n",
    "_,sequence,pairs = testint.strip().split(',')\n",
    "pairs\n",
    "\n",
    "keys = [ list( map(int,pair.split('-')) ) for pair in pairs.split(' ') ]\n",
    "L = len(sequence)\n",
    "a = np.zeros((L,L))\n",
    "for i,j in keys:\n",
    "    if j!=0:\n",
    "        a[i-1,j-1] = 1\n",
    "        a[j-1,i-1] = 1\n",
    "torch.tensor( a, dtype=torch.float32 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(pl.LightningModule):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, padding=padding)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(in_channels, out_channels, kernel_size, padding=padding)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x \n",
    "        out = self.batch_norm1(x)\n",
    "        out = self.conv1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.batch_norm2(x)\n",
    "        out = self.conv2(out)\n",
    "        return out + residual\n",
    "    \n",
    "class RNASecondaryStructurePredictor(pl.LightningModule):\n",
    "    def __init__(self, vocab_size, embedding_dim, num_residual_blocks=1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.residual_blocks = nn.ModuleList([\n",
    "            ResidualBlock(in_channels=2*embedding_dim, out_channels=2*embedding_dim)\n",
    "            for _ in range(num_residual_blocks)\n",
    "        ])\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear = nn.Linear(2*embedding_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        # loss fn\n",
    "        self.loss_fn = nn.BCELoss()\n",
    "        # additional metrics:\n",
    "        self.accuracy = Accuracy(task='binary')\n",
    "        self.precision = Precision(task='binary')\n",
    "        self.recall = Recall(task='binary')\n",
    "        self.f1 = F1Score(task='binary')\n",
    "        self.auc = AUROC(task='binary')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        N, L = x.size()\n",
    "        # (N,L)->(N, L, E)\n",
    "        embeddings = self.embedding(x)  \n",
    "        \n",
    "        # (N, L, E) -> (N, L, L, 2E), outer concatenetion into pair-embedding representation\n",
    "        # first (N,L,E)->(N,L,L,E)\n",
    "        e1 = embeddings.unsqueeze(2).expand(-1, -1, L, -1) \n",
    "        e2 = embeddings.unsqueeze(1).expand(-1, L, -1, -1)  \n",
    "        # concat along last dim, (N,L,L,E)->(N,L,L,2E)\n",
    "        concatenated = torch.cat((e1,e2), dim=-1)\n",
    "        \n",
    "        # (N,L,L,2E) -> (N,2E,L,L), conv2d wants (batch,filters,x,y)\n",
    "        concatenated = concatenated.permute(0, 3, 1, 2)  \n",
    "        # relu\n",
    "        concatenated = self.relu(concatenated)\n",
    "        for block in self.residual_blocks:\n",
    "            concatenated = block(concatenated)\n",
    "        \n",
    "        # go back to (N,2E,L,L) -> (N,L,L,2E)\n",
    "        concatenated = concatenated.permute(0,2,3,1)\n",
    "        # (N,L,L,2E) -> (N,L,L,1)\n",
    "        logits = self.sigmoid( self.linear(concatenated) )\n",
    "        return logits\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x).squeeze(-1) \n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x).squeeze(-1) \n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        self.log('val_loss', loss)\n",
    "        \n",
    "        preds = torch.round(y_hat)\n",
    "        self.log('val_accuracy', self.accuracy(preds, y))\n",
    "        self.log('val_precision', self.precision(preds, y))\n",
    "        self.log('val_recall', self.recall(preds, y))\n",
    "        self.log('val_f1', self.f1(preds, y))\n",
    "        self.log('val_auc', self.auc(y_hat, y))\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x).squeeze(-1) \n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        self.log('test_loss', loss)\n",
    "        \n",
    "        preds = torch.round(y_hat)\n",
    "        self.log('test_accuracy', self.accuracy(preds, y))\n",
    "        self.log('test_precision', self.precision(preds, y))\n",
    "        self.log('test_recall', self.recall(preds, y))\n",
    "        self.log('test_f1', self.f1(preds, y))\n",
    "        self.log('test_auc', self.auc(y_hat, y))\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rna = 'AUCGCUCUGUCGUCCACACUCUAAAAA'\n",
    "tokenizer = {'A': 0, 'C': 1, 'G': 2, 'U': 3}\n",
    "x = torch.tensor([tokenizer[c] for c in rna])\n",
    "x = x.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 27, 27, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RNASecondaryStructurePredictor(vocab_size=4,embedding_dim=10)(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 3, 1, 2, 1, 3, 1, 3, 2, 3, 1, 2, 3, 1, 1, 0, 1, 0, 1, 3, 1, 3, 0, 0,\n",
       "         0, 0, 0]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    " # File paths\n",
    "train_file = \"data_splits/train_ss.csv\"\n",
    "val_file = \"data_splits/validation_ss.csv\"\n",
    "test_file = \"data_splits/test_ss.csv\"\n",
    "\n",
    "# Initialize DataModule\n",
    "data_module = RNADataModule(train_file, val_file, test_file, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[2, 1, 2, 0, 2, 1, 1, 2, 1, 3, 2, 3, 3, 0, 1, 1, 1, 2, 3, 2, 1, 2, 2, 2,\n",
       "          2, 2, 3, 1, 2, 2, 1, 3, 1, 2, 2, 3, 2, 2, 0, 2, 2, 1, 1, 3, 1, 0, 2, 3,\n",
       "          2, 2, 3, 2, 1, 1, 2, 1, 3, 2, 3, 0, 2, 3, 2, 3, 2, 2, 2, 2, 3, 2, 3, 3,\n",
       "          0, 2, 1, 2, 2, 1, 1, 0, 0, 0, 1, 2, 1, 1, 1, 1, 0, 1, 3, 2, 2, 2, 0, 3,\n",
       "          1, 2, 1, 0, 1, 1, 1, 1, 1, 1, 2, 2, 1, 2, 0, 3, 1, 1, 1, 1, 2, 1, 2, 1,\n",
       "          2, 2, 0, 2, 2, 1, 2, 2, 2, 0, 2, 2, 0, 3, 1, 2, 1, 3, 2, 2, 0, 2, 0, 3,\n",
       "          2, 1, 3, 2, 1, 2, 2, 1, 2, 1, 2, 1, 0, 0, 1, 2, 1, 1, 1, 1, 0, 2, 2, 1,\n",
       "          3, 2, 2, 0, 0, 0, 1, 0, 2, 0, 2, 1, 0, 3, 2, 3, 3, 0, 0, 0, 2, 3, 2, 1,\n",
       "          1, 1, 2, 1, 3, 2, 1, 2, 3, 3, 1, 1, 2, 1, 2, 2, 3, 2, 2, 2, 0, 1, 0, 0,\n",
       "          1, 2, 2, 2, 2, 2, 2, 1, 1, 2, 1, 2, 1, 1, 1, 2, 2, 2, 1, 3, 2, 1, 2, 2,\n",
       "          1, 2, 2, 1, 1, 1, 2, 1, 2, 2, 1, 2, 2, 1, 1, 0, 0, 1, 2, 1, 1, 2, 0, 2,\n",
       "          1, 3, 3, 3, 3]]),\n",
       " tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_batch = next(iter(data_module.train_dataloader()))\n",
    "test_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/fgloblek/.pyenv/versions/3.10.8/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:617: UserWarning: Checkpoint directory /home/fgloblek/Documents/Programiranje/rna/logs/RNASecondaryStructurePredictor exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "   | Name            | Type            | Params\n",
      "-----------------------------------------------------\n",
      "0  | embedding       | Embedding       | 40    \n",
      "1  | residual_blocks | ModuleList      | 7.3 K \n",
      "2  | relu            | ReLU            | 0     \n",
      "3  | linear          | Linear          | 21    \n",
      "4  | sigmoid         | Sigmoid         | 0     \n",
      "5  | loss_fn         | BCELoss         | 0     \n",
      "6  | accuracy        | BinaryAccuracy  | 0     \n",
      "7  | precision       | BinaryPrecision | 0     \n",
      "8  | recall          | BinaryRecall    | 0     \n",
      "9  | f1              | BinaryF1Score   | 0     \n",
      "10 | auc             | BinaryAUROC     | 0     \n",
      "-----------------------------------------------------\n",
      "7.4 K     Trainable params\n",
      "0         Non-trainable params\n",
      "7.4 K     Total params\n",
      "0.030     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44861997a8ca4d61aabd62a6c79593b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fgloblek/.pyenv/versions/3.10.8/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:438: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/fgloblek/.pyenv/versions/3.10.8/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:438: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f564b134a8645ea86ac728eb9a8c8e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f33ad0e65df4b0cb988f7afe9d6e486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 0.026\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bf9f51986f54111886fbaeeea63c057",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.026\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91bb74001f9d49e7a7ad1601d996e372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.026\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ee3270855df4da29da1fdc475ccb8d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6c5261df5a541b492d7c67e1b5ed23e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.026\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "613859ee3d4c4ef291ae8e8b75db3935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.026\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa01a11b58774d8f9db21d1400d8375b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53d0ad397fe14cdd8b53fce9cc39035f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_loss did not improve in the last 2 records. Best score: 0.026. Signaling Trainer to stop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model saved at: /home/fgloblek/Documents/Programiranje/rna/logs/RNASecondaryStructurePredictor/epoch=5-val_loss=0.03.ckpt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "RNASecondaryStructurePredictor.__init__() missing 2 required positional arguments: 'vocab_size' and 'embedding_dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m best_model_path \u001b[38;5;241m=\u001b[39m checkpoint_callback\u001b[38;5;241m.\u001b[39mbest_model_path\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest model saved at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_model_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 31\u001b[0m best_model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mload_from_checkpoint(best_model_path)\n\u001b[1;32m     33\u001b[0m model_metrics \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mtest(best_model, datamodule\u001b[38;5;241m=\u001b[39mdata_module)\n\u001b[1;32m     34\u001b[0m model_metrics\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/lib/python3.10/site-packages/pytorch_lightning/core/module.py:1537\u001b[0m, in \u001b[0;36mLightningModule.load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m   1457\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   1458\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_from_checkpoint\u001b[39m(\n\u001b[1;32m   1459\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1464\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m   1465\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[1;32m   1466\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1467\u001b[0m \u001b[38;5;124;03m    Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint\u001b[39;00m\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;124;03m    it stores the arguments passed to ``__init__``  in the checkpoint under ``\"hyper_parameters\"``.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1535\u001b[0m \u001b[38;5;124;03m        y_hat = pretrained_model(x)\u001b[39;00m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1537\u001b[0m     loaded \u001b[38;5;241m=\u001b[39m \u001b[43m_load_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1538\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1539\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1540\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1541\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhparams_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1542\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1544\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1545\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Self, loaded)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:91\u001b[0m, in \u001b[0;36m_load_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _load_state(\u001b[38;5;28mcls\u001b[39m, checkpoint, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, pl\u001b[38;5;241m.\u001b[39mLightningModule):\n\u001b[0;32m---> 91\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43m_load_state\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m state_dict:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.8/lib/python3.10/site-packages/pytorch_lightning/core/saving.py:144\u001b[0m, in \u001b[0;36m_load_state\u001b[0;34m(cls, checkpoint, strict, **cls_kwargs_new)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cls_spec\u001b[38;5;241m.\u001b[39mvarkw:\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;66;03m# filter kwargs according to class init unless it allows any argument via kwargs\u001b[39;00m\n\u001b[1;32m    142\u001b[0m     _cls_kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m _cls_kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m cls_init_args_name}\n\u001b[0;32m--> 144\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_cls_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, pl\u001b[38;5;241m.\u001b[39mLightningModule):\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;66;03m# give model a chance to load something\u001b[39;00m\n\u001b[1;32m    148\u001b[0m     obj\u001b[38;5;241m.\u001b[39mon_load_checkpoint(checkpoint)\n",
      "\u001b[0;31mTypeError\u001b[0m: RNASecondaryStructurePredictor.__init__() missing 2 required positional arguments: 'vocab_size' and 'embedding_dim'"
     ]
    }
   ],
   "source": [
    "model = RNASecondaryStructurePredictor(vocab_size=4,embedding_dim=10,num_residual_blocks=1)\n",
    "\n",
    "checkpoint_dir = os.path.join('logs', model.__class__.__name__)\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",      \n",
    "    mode=\"min\",              \n",
    "    save_top_k=1,           \n",
    "    dirpath=checkpoint_dir, \n",
    "    filename=\"{epoch}-{val_loss:.2f}\"  \n",
    ")\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    patience=2, \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(\"logs\", name=\"ss_predict\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=20,\n",
    "    callbacks=[checkpoint_callback, early_stopping],\n",
    "    logger=csv_logger)\n",
    "\n",
    "trainer.fit(model, datamodule=data_module)\n",
    "best_model_path = checkpoint_callback.best_model_path\n",
    "print(f\"Best model saved at: {best_model_path}\")\n",
    "\n",
    "best_model = model.load_from_checkpoint(best_model_path)\n",
    "\n",
    "model_metrics = trainer.test(best_model, datamodule=data_module)\n",
    "model_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fgloblek/.pyenv/versions/3.10.8/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:438: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbcdaeac17bf46e29b2c20d7ac7290f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fgloblek/.pyenv/versions/3.10.8/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    }
   ],
   "source": [
    "best_model = model.load_from_checkpoint(best_model_path,vocab_size=4,embedding_dim=10,num_residual_blocks=1)\n",
    "\n",
    "model_metrics = trainer.test(best_model, datamodule=data_module)\n",
    "model_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0010],\n",
       "         [0.0047],\n",
       "         [0.0037],\n",
       "         [0.0041],\n",
       "         [0.0033]],\n",
       "\n",
       "        [[0.0052],\n",
       "         [0.0037],\n",
       "         [0.0055],\n",
       "         [0.0042],\n",
       "         [0.0018]],\n",
       "\n",
       "        [[0.0033],\n",
       "         [0.0050],\n",
       "         [0.0074],\n",
       "         [0.0056],\n",
       "         [0.0037]],\n",
       "\n",
       "        [[0.0039],\n",
       "         [0.0038],\n",
       "         [0.0056],\n",
       "         [0.0042],\n",
       "         [0.0025]],\n",
       "\n",
       "        [[0.0032],\n",
       "         [0.0014],\n",
       "         [0.0035],\n",
       "         [0.0027],\n",
       "         [0.0028]]], grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = {'A': 0, 'C': 1, 'G': 2, 'U': 3}\n",
    "sequence = 'ACGUG'\n",
    "test =  torch.tensor([tokenizer[c] for c in sequence])\n",
    "\n",
    "model(test.unsqueeze(0)).squeeze(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
