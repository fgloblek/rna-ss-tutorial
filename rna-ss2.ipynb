{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os \n",
    "from collections import defaultdict\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need the functions from the previous task in order to parse pseudoknots. \n",
    "\n",
    "The logic, very shortly is:\n",
    "\n",
    "Send the arc diagram of the sequence to its shape, assign brackets, and then pullback the brackets.\n",
    "$${\\rm s}:{\\rm Arcs}\\to {\\rm Shapes}, \\quad {\\rm bracketing}:{\\rm Shapes}\\to {\\rm Brackets} $$\n",
    "$$\\Rightarrow {\\rm bracketing}^*:{\\rm Arcs}\\to{\\rm Brackets}$$\n",
    "such that ${\\rm bracketing}^*(pair) = {\\rm bracketing}(s(pair))$.\n",
    "\n",
    "A bit more verbose:\n",
    "1. Consider the rainbow graph or arc diagram (https://en.wikipedia.org/wiki/Arc_diagram) of the primary structure, with the arcs drawn in the upper half plane.\n",
    "The nodes are elements of the sequence, and they have an arc connecting them if they're paired.\n",
    "2. Create the \"shape\", which means that first all the isolated, unconnected nodes are removed. Then, parallel pairs, which is a sequence like ${(A + i, B - i)}_{i\\in[0..n]}$ are collapsed or glued to a single ark.\n",
    "3. Finally, use the shape to assign brackets. Isolated arcs in the \"shape\" all get \"()\" pairs. Overlapping arcs get (),<>,[],etc. in a predefined sequence\n",
    "4. And then make sure everything in the preimage of map 2 has the same bracket (or \"no bracket\" ie a dot for the unassigned isolated nodes). \n",
    "\n",
    "\n",
    "How I implemented the logic was a bit different, in the sense that after step 1., I didn't create the shape immediately, but I first focused on connected components of overlapping/intersecting arcs. \n",
    "\n",
    "Then, in each component, I collapsed everything to the shape and assigned brackets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune0arcs(pairs):\n",
    "    \"\"\"\n",
    "    Remove 0-arcs (unconnected pairs) from a list of pairs.\n",
    "    Rename the surviving pairs so they're sequential\n",
    "    E.g. (1,x),(2,0),(3,y) -> (f(1),f(x)), --this one died--, (f(3),f(y))\n",
    "    and f(1)=1, f(3)=2, etc.\n",
    "    Return the new pairs and the map f.\n",
    "    \"\"\"\n",
    "    first_pairs = [a for a,b in pairs if b!=0]\n",
    "    rename_pairs_map = { a:(i+1) for i,a in enumerate(sorted(first_pairs))}\n",
    "    renamed_pairs = [(rename_pairs_map[a],rename_pairs_map[b]) for a,b in pairs if b!=0]\n",
    "    \n",
    "    return renamed_pairs, rename_pairs_map\n",
    "\n",
    "def build_overlap_graph(pairs):\n",
    "    \"\"\"\n",
    "    In a rainbow graph of a primary structure\n",
    "    where pairs are connected by arcs,\n",
    "    return a graph in the form of an adjancency list,\n",
    "    which consists of those pairs whose arcs intersect (this is the pseudoknot condition).\n",
    "    \"\"\"\n",
    "    overlaps = {}\n",
    "    for i, j in pairs:\n",
    "        if i < j:  \n",
    "            overlaps[(i, j)] = []\n",
    "    for (i1, j1) in pairs:\n",
    "        for (i2, j2) in pairs:\n",
    "        # pseudoknot/overlapping arcs condition\n",
    "            if (i1 < i2 < j1 < j2) or (i2 < i1 < j2 < j1):\n",
    "                overlaps[(i1, j1)].append((i2, j2))\n",
    "                overlaps[(i2, j2)].append((i1, j1))\n",
    "    \n",
    "    return overlaps\n",
    "\n",
    "def connected_components(graph):\n",
    "    \"\"\"Standard dfs algorithm to find connected components in a graph\"\"\"\n",
    "    visited = set()\n",
    "    components = []\n",
    "\n",
    "    def dfs(node, component):\n",
    "        visited.add(node)\n",
    "        component.append(node)\n",
    "        for neighbor in graph.get(node, []):\n",
    "            if neighbor not in visited:\n",
    "                dfs(neighbor, component)\n",
    "    \n",
    "    for node in graph:\n",
    "        if node not in visited:\n",
    "            component = []\n",
    "            dfs(node, component)\n",
    "            components.append(component)\n",
    "    \n",
    "    return components\n",
    "\n",
    "def make_dot_bracket(sequence, pairs):\n",
    "    \"\"\"\n",
    "    Creates a dot-bracketed string from a sequence and a list of paris.\n",
    "    First, remove 0-arcs (unconnected pairs) and rename the pruned sequence.\n",
    "    Then, build a graph, so that nodes are pairs, and edges connect \n",
    "    only those pairs whose arcs intersect (so their topology is nontrivial). \n",
    "    Then, focus on connected components of this graph. \n",
    "    Give all parallel arcs the same bracket.\n",
    "    \"\"\"\n",
    "    renamed_pairs, rename_pairs_map = prune1arcs(pairs)\n",
    "\n",
    "    inverse_rename_pairs_map = { v:k for k,v in rename_pairs_map.items()}\n",
    "\n",
    "    graph = build_overlap_graph(renamed_pairs)\n",
    "    components = connected_components(graph)\n",
    "    openbrackets = '(<[{ACEGIKMOQSU'\n",
    "    closedbrackets = ')>]}BDFHJLNPRTV'\n",
    "    bracketdict = defaultdict(lambda:'.')\n",
    "\n",
    "    for component in components:\n",
    "    \n",
    "        # e.g. for the component [(1, 6), (2, 7), (4, 9), (5, 8)]\n",
    "        \n",
    "        # first sort according to first element\n",
    "        sorted_renamed_component = sorted( [ (a,b) for a,b in component if a<b], key=lambda x:x[0])\n",
    "        \n",
    "        # next, collapse all parallel pairs \n",
    "        # which are those like (i,j) and (i+1,j-1) for any i and j\n",
    "        # to the first pair in such a sequence.\n",
    "        \n",
    "        collapsing_parallels_map = {}\n",
    "        current = sorted_renamed_component[0] # start from first pair\n",
    "        for k,pair in enumerate(sorted_renamed_component):\n",
    "            # iterate through pairs and send all parallel pairs to the same target pair \n",
    "            if k>1 and sorted_renamed_component[k][1] != sorted_renamed_component[k-1][1] - 1:\n",
    "                # if not parallel to previous pair, this is a target \n",
    "                current = pair\n",
    "            collapsing_parallels_map[pair] = current\n",
    "            \n",
    "        # for our example, \n",
    "        # collapsing_parallels_map is {(1, 6): (1, 6), (2, 7): (1, 6), (4, 9): (4, 9), (5, 8): (4, 9)}\n",
    "        \n",
    "        # look at the preimage of the collapsing_parallels map\n",
    "        # e.g. {(1, 6): (1, 6), (2, 7): (1, 6), (4, 9): (4, 9), (5, 8): (4, 9)}\n",
    "        d = [ (n, [k for k in collapsing_parallels_map.keys() if collapsing_parallels_map[k] == n]) for n in set(collapsing_parallels_map.values()) ]\n",
    "\n",
    "        # assign the same bracket to everything in a given preimage\n",
    "        for k,(target,preimage) in enumerate(sorted(d,key=lambda x:x[1][0])):\n",
    "            for image in preimage:\n",
    "                bracketdict[ inverse_rename_pairs_map[image[0]]]= openbrackets[k]\n",
    "                bracketdict[ inverse_rename_pairs_map[image[1]]]= closedbrackets[k]\n",
    "           \n",
    "    return ''.join( bracketdict[i] for i in range(1,len(sequence)+1))\n",
    "\n",
    "def ct2dbn(source_filename,target_filename):\n",
    "    \"\"\"\n",
    "    Converts .ct file to .dbn file. \n",
    "    \"\"\"\n",
    "    ct_dir = os.path.join(os.getcwd(),'archiveII')\n",
    "    file_loc = os.path.join(ct_dir,source_filename)\n",
    "\n",
    "    print('reading:')\n",
    "    with open(file_loc,'r') as file:\n",
    "        lines = file.readlines()\n",
    "        name = lines[0].split()[1]\n",
    "        print( '> ' + name )\n",
    "        sequence = ''.join( line.split()[1] for line in lines[1:] )\n",
    "\n",
    "        print( ' '.join( line.split()[0] for line in lines[1:] ) )\n",
    "        print( ' '.join( line.split()[4] for line in lines[1:] ) )\n",
    "        pairings = [ ( line.split()[0],line.split()[4]) for line in lines[1:]]\n",
    "        \n",
    "    pairs = [ (int(a),int(b)) for a,b in pairings ]\n",
    "    \n",
    "    print('writing...')\n",
    "    with open( os.path.join(ct_dir,target_filename),'w') as file:\n",
    "        file.write('>'+name)\n",
    "        file.write(sequence+'\\n')\n",
    "        file.write(make_dot_bracket(sequence, pairs))\n",
    "    print('wrote:')\n",
    "    with open( os.path.join(ct_dir,target_filename),'r') as file:\n",
    "        print(file.read())\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ct 2 fasta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ct_to_fasta(input_folder, output_fasta):\n",
    "    \"\"\"\n",
    "    Converts .ct files in a folder to a FASTA file without duplicate sequences.\n",
    "    \"\"\"\n",
    "    sequences = {}  # duplicate sequences will be removed\n",
    "\n",
    "    for file_name in os.listdir(input_folder):\n",
    "        if file_name.endswith(\".ct\"):\n",
    "            file_path = os.path.join(input_folder, file_name)\n",
    "            with open(file_path, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "                \n",
    "                sequence = ''.join( line.split()[1] for line in lines[1:] )\n",
    "                \n",
    "                if sequence not in sequences:\n",
    "                    # name unique sequences according to file name\n",
    "                    sequences[sequence] = file_name  \n",
    "\n",
    "    # write to FASTA\n",
    "    with open(output_fasta, 'w') as out_f:\n",
    "        for i, (seq, source) in enumerate(sequences.items(), start=1):\n",
    "            out_f.write(f\">sequence{i} {source}\\n\")\n",
    "            out_f.write(f\"{seq}\\n\")\n",
    "\n",
    "    print(f\"FASTA file created: {output_fasta}, with {len(sequences)} unique sequences.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_to_fasta(\"archiveII\", \"archiveII.fasta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will also need to encode \"has_pseudoknot\" as a target variable for all these sequences.\n",
    "\n",
    "Use the make_dot_bracket to find the dot-bracket, then flag if a \"<\" exists in the dot-bracket. This is a bit of an overkill, the overlaps are enough, but I'm lazy to rewrite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasta_to_pk(input_folder, input_fasta, output):\n",
    "    \n",
    "    with open(input_fasta, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    lines_to_write = ''\n",
    "    for line in lines:\n",
    "        if line.startswith('>'):\n",
    "            file_name = line.split()[1]\n",
    "            file_path = os.path.join(input_folder, file_name)\n",
    "            with open(file_path, 'r') as f:\n",
    "                ctlines = f.readlines()\n",
    "                sequence = ''.join( ctline.split()[1] for ctline in ctlines[1:] )\n",
    "                pairings = [ ( ctline.split()[0],ctline.split()[4]) for ctline in ctlines[1:]]\n",
    "        \n",
    "            pairs = [ (int(a),int(b)) for a,b in pairings ]\n",
    "            dot_bracket = make_dot_bracket(sequence,pairs)\n",
    "            has_pk = 1 if '<' in dot_bracket else 0\n",
    "            lines_to_write += line.split()[0][1:] +' '+ str(has_pk) + '\\n'\n",
    "                \n",
    "    with open(output,'w') as f:\n",
    "        f.write(lines_to_write)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "fasta_to_pk('archiveII', 'archiveII.fasta', 'pk.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_clusters(tsv_file, output_dir, train_frac=0.7, val_frac=0.15, test_frac=0.15, seed=42):\n",
    "    \"\"\"\n",
    "    Splits RNA sequences into training, validation, and test sets based on clusters\n",
    "    from MMseqs2.\n",
    "    \n",
    "    Parameters:\n",
    "    - tsv_file (str): Path to the MMseqs2 output .tsv file with clusters.\n",
    "    - output_dir (str): Directory to save the split files.\n",
    "    - train_frac (float): Fraction of clusters for training set.\n",
    "    - val_frac (float): Fraction of clusters for validation set.\n",
    "    - test_frac (float): Fraction of clusters for test set.\n",
    "    - seed (int): Random seed for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Read the .tsv file and assign sequences to clusters\n",
    "    clusters = {}\n",
    "    with open(tsv_file, 'r') as f:\n",
    "        for line in f:\n",
    "            cluster_id, sequence_id = line.strip().split(\"\\t\")\n",
    "            if cluster_id not in clusters:\n",
    "                clusters[cluster_id] = []\n",
    "            clusters[cluster_id].append(sequence_id)\n",
    "    \n",
    "    # Shuffle clusters into train,val,test sets of clusters\n",
    "    cluster_ids = list(clusters.keys())\n",
    "    random.shuffle(cluster_ids)\n",
    "    \n",
    "    num_clusters = len(cluster_ids)\n",
    "    train_end = int(train_frac * num_clusters)\n",
    "    val_end = train_end + int(val_frac * num_clusters)\n",
    "    \n",
    "    train_clusters = cluster_ids[:train_end]\n",
    "    val_clusters = cluster_ids[train_end:val_end]\n",
    "    test_clusters = cluster_ids[val_end:]\n",
    "    \n",
    "    # Assign the actual sequences to those clusters\n",
    "    train_set = [seq for cluster in train_clusters for seq in clusters[cluster]]\n",
    "    val_set = [seq for cluster in val_clusters for seq in clusters[cluster]]\n",
    "    test_set = [seq for cluster in test_clusters for seq in clusters[cluster]]\n",
    "    \n",
    "    # Save to files\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    with open(os.path.join(output_dir, \"train.txt\"), 'w') as f:\n",
    "        f.write(\"\\n\".join(train_set) + \"\\n\")\n",
    "    with open(os.path.join(output_dir, \"validation.txt\"), 'w') as f:\n",
    "        f.write(\"\\n\".join(val_set) + \"\\n\")\n",
    "    with open(os.path.join(output_dir, \"test.txt\"), 'w') as f:\n",
    "        f.write(\"\\n\".join(test_set) + \"\\n\")\n",
    "    \n",
    "    print(f\"Data split, files saved in {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_clusters(\"clusterRes_cluster.tsv\", \"data_splits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want a slightly different input for my models.\n",
    "\n",
    "A .csv with rows like\n",
    "\n",
    "[sequenceID],[letters],[has_pseudoknot]\n",
    "\n",
    "sequence1,ACGUCUGU...,0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_csv(target_file, input_fasta, split_dir, split_file, output):\n",
    "    with open( os.path.join(os.getcwd(),split_dir,split_file), 'r') as f:\n",
    "        split_set = [line.strip() for line in f.readlines()]\n",
    "    \n",
    "    with open(target_file,'r') as f:\n",
    "        lines = f.readlines()\n",
    "        target_dict = dict([line.split()  for line in lines ])\n",
    "        \n",
    "    sequence_dict = {}\n",
    "    with open(input_fasta,'r') as f:\n",
    "        for line in f.readlines():\n",
    "            if line.startswith('>'):\n",
    "                seq_name = line.split()[0][1:]\n",
    "            else: \n",
    "                sequence = line.strip()\n",
    "                sequence_dict[seq_name]=sequence\n",
    "                \n",
    "    with open( os.path.join(os.getcwd(),split_dir,output),'w') as f:\n",
    "        for seq in split_set:\n",
    "            f.write(seq+','+sequence_dict[seq]+','+target_dict[seq]+'\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_csv('pk.txt', 'archiveII.fasta', 'data_splits','test.txt', 'test.csv')\n",
    "make_csv('pk.txt', 'archiveII.fasta', 'data_splits','train.txt', 'train.csv')\n",
    "make_csv('pk.txt', 'archiveII.fasta', 'data_splits','validation.txt', 'validation.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
